{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWZ5WpWmhAE9"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
        "    <img style=\"float: right;\" src=\"imgs/OST.png\" width=260, height=130>\n",
        "</div>\n",
        "<div style=\"text-align: center;\">\n",
        "    <h1>Learning a Humanoid to Standup with Reinforcement Learning</h1>\n",
        "    <h2>Christoph Landolt</h2>\n",
        "    <h3>20. June 2024</h3>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n",
        "\n",
        "**Task Description**\n",
        "\n",
        "In this Notebook, the HumanoidStandup Task is solved. This task involves training a reinforcement learning agent to control a humanoid model consisting of 15 body parts to transition from a horizontal lying position to a standing posture.\n",
        "This task is especially challenging because the agent interacts with a high-dimensional action space of 17 actions corresponding to joint angles and velocities and observes a state space of 376 dimensions, which includes information about the Humanoid's position, velocity, and angular velocity.\n",
        "The task is considered solved when the Humanoid successfully stands up without falling over. This demonstrates the agent's ability to handle complex, high-dimensional control tasks.\n",
        "\n",
        "**Project Scope**\n",
        "\n",
        "The aim of this project is to evaluate the best reinforcement learning algorithm for the HumaniudStandup task and then to optimize the parameters of this algorithm.\n",
        "Subsequently, reward shaping will be carried out and it will be shown how several agents can be trained in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Objective and approach\n",
        "\n",
        "**Objective**\n",
        "This project aims to train a horizontal-lying humanoid robot, simulated in the Mujoco physics engine, to stand up independently.\n",
        "\n",
        "**Approach**\n",
        "\n",
        "The project is carried out in the following steps:\n",
        "1) Comparison of the following RL algorithms for this task: PPO, SAC, TD3 and A2C with the default parameters by comparing the reward. The choice of the RL algorithm will be verified with the current literature.\n",
        "2) Tuning the parameters of the selected RL algorithm\n",
        "3) Creating and registering a custum environment with adjusted reward\n",
        "4) Parallel training of the agent\n",
        "5) Discussion of the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Python Library Requirements\n",
        "\n",
        "This project requires the following Python libraries:\n",
        "\n",
        "- mujoco\n",
        "- gymnasium\n",
        "- stable-baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Documentation of the Mujoco Humanoid simulation (Adapted from [Gymnasium Documentation](https://gymnasium.farama.org/environments/mujoco/humanoid_standup/))\n",
        "The environment which is described in [Paper](https://ieeexplore.ieee.org/document/6386025) simulates a human consisting of two legs, two arms and a Torso.\n",
        "\n",
        "The environment is designet to learn a humanoid to stand up or to lern how to walk.\n",
        "\n",
        "### Overview\n",
        "| Description | Information |\n",
        "|----------|----------|\n",
        "| Action Space   | ``` Box(-0.4, 0.4, (17,), float32) ```   |\n",
        "| Observation Space    | ```Box(-inf, inf, (348,), float64)```     |\n",
        "| import learning to Walk    | \t```gymnasium.make(\"Humanoid-v4\")```     |\n",
        "| import standup   | ```gymnasium.make(\"HumanoidStandup-v4\")```     |\n",
        "\n",
        "\n",
        "### Action Space\n",
        "An action represents the torques applied at the hinge joints.\n",
        "\n",
        "![imgs/humanoid.png](imgs/humanoid.png)\n",
        "\n",
        "| Num | Name  | Joint | Type (Unit) | \n",
        "|----------|----------|----------|----------|\n",
        "| 0 | abdomen_y | hinge | torque (N m) |\n",
        "| 1 | abdomen_z | hinge | torque (N m) | \n",
        "| 2 | abdomen_x | hinge | torque (N m) |\n",
        "| 3 | right_hip_x | hinge | torque (N m) |\n",
        "| 4 | right_hip_z | hinge | torque (N m) |\n",
        "| 5 | right_hip_y | hinge | torque (N m) |\n",
        "| 6 | right_knee | hinge | torque (N m) | \n",
        "| 7 | left_hip_x  | hinge | torque (N m) |\n",
        "| 8 | left_hip_z | hinge | torque (N m) |\n",
        "| 9 | left_hip_y | hinge | torque (N m) | \n",
        "| 10 | left_knee | hinge | torque (N m) | \n",
        "| 11 | right_shoulder1 | hinge | torque (N m) | \n",
        "| 12 | right_shoulder2 | hinge | torque (N m) | \n",
        "| 13 | right_elbow | hinge | torque (N m) | \n",
        "| 14 | left_shoulder1 | hinge | torque (N m) | \n",
        "| 15 | left_shoulder2 | hinge | torque (N m) | \n",
        "| 16 | left_elbow | hinge | torque (N m) | \n",
        "\n",
        "### Observation Space\n",
        "The observation space for the Gymnasium HumanoidStandup task is a 376-dimensional continuous space. This high-dimensional space captures a wide range of information about the state of the humanoid model, which consists of 15 body parts.\n",
        "\n",
        "The state vector includes the following information:\n",
        "\n",
        "- Position and velocity of the humanoid's center of mass\n",
        "- Position, velocity, and angular velocity of each of the humanoid's body parts\n",
        "- Joint angles and velocities\n",
        "- Contact forces\n",
        "- Boolean indicators for whether each body part is in contact with the ground\n",
        "\n",
        "This rich state representation allows the reinforcement learning agent to have a comprehensive understanding of the current state of the humanoid and its environment, which is crucial for learning to balance and coordinate the humanoid's movements to stand up from a prone position.\n",
        "\n",
        "A detailed overview can be found under this [Link](https://gymnasium.farama.org/environments/mujoco/humanoid_standup/).\n",
        "\n",
        "### Rewards\n",
        "**Standup-Task**\n",
        "\n",
        "The total reward is: reward = uph_cost + 1 - quad_ctrl_cost - quad_impact_cost.\n",
        "- uph_cost: A reward for moving up\n",
        "- quad_ctrl_cost: A negative reward to penalize the Humanoid for taking actions that are too large.\n",
        "- impact_cost: A negative reward to penalize the Humanoid if the external contact forces are too large.\n",
        "\n",
        "A detailed overview can be found under this [Link](https://gymnasium.farama.org/environments/mujoco/humanoid_standup/).\n",
        "\n",
        "**Learning to Walk-Task**\n",
        "\n",
        "The reward function for the Humanoid learning to walk task is composed of four parts:\n",
        "\n",
        "1. Healthy Reward: A fixed reward given at every timestep that the humanoid is alive.\n",
        "\n",
        "2. Forward Reward: A reward for moving forward, calculated as `forward_reward_weight * (average center of mass before action - average center of mass after action) / dt`. Here, `dt` is the time between actions, dependent on the `frame_skip` parameter.\n",
        "\n",
        "3. Control Cost: A penalty for large control forces, calculated as `ctrl_cost_weight * sum(control^2)`.\n",
        "\n",
        "4. Contact Cost: A penalty for large external contact forces.\n",
        "\n",
        "The total reward is calculated as `reward = healthy_reward + forward_reward - ctrl_cost - contact_cost`.\n",
        "\n",
        "A detailed overview can be found under this [Link](https://gymnasium.farama.org/environments/mujoco/humanoid/#)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Description of the used Algorithms\n",
        "The following 3 agents are being evaluated for this project:\n",
        "\n",
        "### 5.1 Soft Actor-Critic (SAC) (Theory adapted from [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/sac.html#id2) implemented with [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html))\n",
        "The Soft Actor-Critic was introduced in the following Paper: [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290)\n",
        "\n",
        "The Soft Actor-Critic (SAC) is an off-policy reinforcement learning algorithm that optimizes a stochastic policy in an entropy-regularized framework. The objective function is given by `J(œÄ) = ùîº[‚àë_t Œ≥^t (r(s_t, a_t) + Œ± H(œÄ(¬∑|s_t)))]`, where `r(s_t, a_t)` is the reward, `H(œÄ(¬∑|s_t))` is the entropy of the policy `œÄ` at state `s_t`, `Œ≥` is the discount factor, and `Œ±` is the temperature parameter that determines the trade-off between exploration and exploitation.\n",
        "\n",
        "The SAC algorithm uses two Q-functions to reduce overestimation bias, and learns the policy by minimizing the KL divergence from an exponential of the Q-function. The policy update rule is `œÄ^* = argmin_œÄ D_KL(œÄ(¬∑|s_t) || exp(Q(s_t, ¬∑)/Œ±))`.\n",
        "\n",
        "The Q-functions are updated by minimizing the Bellman residual, `ùîº[(Q(s_t, a_t) - (r(s_t, a_t) + Œ≥ ùîº[Q(s_{t+1}, œÄ(s_{t+1})) - Œ± log œÄ(a_{t+1}|s_{t+1})]))^2]`.\n",
        "\n",
        "SAC has been shown to achieve state-of-the-art performance on a range of continuous control tasks due to its stability and efficiency.\n",
        "\n",
        "\n",
        "### 5.2 Proximal Policy Optimization (PPO) (Theory adapted from [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/ppo.html) implemented with [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html))\n",
        "The Proximal Policy Optimization Algorithms was introduced in the following Paper: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n",
        "\n",
        "Proximal Policy Optimization (PPO) is a type of policy gradient method for reinforcement learning that aims to improve sample efficiency and ease of implementation. It uses a surrogate objective function to take multiple optimization steps on the same batch of data, while limiting the policy update to prevent it from deviating too much from the current policy. This is achieved by adding a penalty term to the objective function, which discourages large policy changes.\n",
        "\n",
        "The objective function in PPO is:\n",
        "\n",
        "$$L^{CLIP}(\\theta) = \\hat{E}_t[min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$\n",
        "\n",
        "where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio of the new and old policies, $\\hat{A}_t$ is the estimated advantage at time $t$, and $\\epsilon$ is a hyperparameter that controls the degree of policy change.\n",
        "\n",
        "PPO strikes a balance between sample complexity, ease of implementation, and computational cost, making it a popular choice for many reinforcement learning tasks.\n",
        "\n",
        "### 5.3 Advantage Actor Critic (A2C) (Theory adapted from [Hugging Face](https://huggingface.co/blog/deep-rl-a2c) implemented with [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html))\n",
        "\n",
        "The Advantage Actor Critic Algorithms was introduced in the following Paper: [Asynchronous Methods for Deep Reinforcement Learning\n",
        "](https://arxiv.org/abs/1602.01783)\n",
        "\n",
        "Advantage Actor-Critic (A2C) is a type of policy gradient method for reinforcement learning that combines the strengths of both actor-critic and advantage methods. The actor is the policy that the agent uses to interact with the environment, while the critic estimates the value function to help the actor improve its policy. The advantage function, defined as $A(s, a) = Q(s, a) - V(s)$, measures how much better an action is compared to the average action at that state.\n",
        "\n",
        "The objective function in A2C is:\n",
        "\n",
        "$$L(\\theta) = \\hat{E}_t[\\log \\pi_\\theta(a_t|s_t) \\hat{A}_t]$$\n",
        "\n",
        "where $\\pi_\\theta(a_t|s_t)$ is the policy (actor), $\\hat{A}_t$ is the estimated advantage at time $t$, and $\\theta$ are the parameters of the policy.\n",
        "\n",
        "A2C uses synchronous updates for efficiency and stability. It is an online algorithm, learning while the agent interacts with the environment. It's widely used due to its balance between computational efficiency and learning performance.\n",
        "\n",
        "### 5.4 TD3: Twin Delayed DDPG (Deep Deterministic Policy Gradient) (Theory adapted from [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/td3.html#id1) implemented with [Stable-Baselines3](https://stable-baselines.readthedocs.io/en/master/modules/td3.html?highlight=TD3))\n",
        "\n",
        "The Twin Delayed DDPG Algorithms was introduced in the following Paper: [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/pdf/1802.09477)\n",
        "\n",
        "clandolt\n",
        "Describe the Twin Delayed DDPG Algorithms im max 6 Sentences. Include Formulas. Output as Jupyter Notebook Markdown\n",
        "\n",
        "GitHub Copilot\n",
        "Twin Delayed Deep Deterministic Policy Gradient (TD3) is an algorithm for reinforcement learning that improves upon the Deep Deterministic Policy Gradient (DDPG) method. TD3 addresses the overestimation bias in Q-value estimation by maintaining two Q-functions and using the smaller of the two Q-values to update the policy. It also introduces a delay in the policy update to reduce the variance of policy and Q-value updates.\n",
        "\n",
        "The objective function for the Q-functions in TD3 is:\n",
        "\n",
        "$$L(\\theta_i) = \\hat{E}{s,a,r,s'\\sim D}[(Q_i(s,a|\\theta_i) - (r + \\gamma min{j=1,2}Q(s',\\pi(s'|\\phi_{old})|\\theta_{j_{old}})))^2]$$\n",
        "\n",
        "where $Q_i(s,a|\\theta_i)$ are the Q-functions, $r$ is the reward, $\\gamma$ is the discount factor, $\\pi(s'|\\phi_{old})$ is the old policy, and $\\theta_{j_{old}}$ are the old parameters of the Q-functions.\n",
        "\n",
        "The objective function for the policy is:\n",
        "\n",
        "$$L(\\phi) = \\hat{E}_{s\\sim D}[-Q_1(s,\\pi(s|\\phi)|\\theta_1)]$$\n",
        "\n",
        "where $Q_1(s,\\pi(s|\\phi)|\\theta_1)$ is one of the Q-functions.\n",
        "\n",
        "TD3 is an online algorithm, learning while the agent interacts with the environment. It's known for its stability and efficiency in learning continuous control tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiPrsLZShX3t",
        "outputId": "d6243f84-df22-45c7-e829-1656409a14a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/landolt/DeepRL/.venv/lib/python3.10/site-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "/home/landolt/DeepRL/.venv/lib/python3.10/site-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "/home/landolt/DeepRL/.venv/lib/python3.10/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "from stable_baselines3 import SAC, TD3, A2C, PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choice of the best RL Algorithm\n",
        "1. The basic idea is to test several RL algorithms with the standard parameters in parallel and to monitor the training progress using the reward.\n",
        "2. Subsequently, the algorithm that makes the fastest training progress is to be implemented and tuned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directories to hold models and logs for the tensor board\n",
        "model_dir = \"models\"\n",
        "log_dir = \"logs\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(log_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define training parameters\n",
        "TIMESTEPS = 25000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(env, humanoid_training_algo):\n",
        "    match humanoid_training_algo:\n",
        "        case 'PPO':\n",
        "            model = PPO('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case 'SAC':\n",
        "            model = SAC('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case 'TD3':\n",
        "            model = TD3('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case 'A2C':\n",
        "            model = A2C('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case _:\n",
        "            print('Algorithm not found')\n",
        "            return\n",
        "    iters = 0\n",
        "    while True:\n",
        "        iters += 1\n",
        "\n",
        "        model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False)\n",
        "        model.save(f\"{model_dir}/{humanoid_training_algo}_{TIMESTEPS*iters}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train the Model**\n",
        "After one Algorithm performs clearly better than the others interrupt the training in the Jupyter Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"None\")\n",
        "algorithms = ['PPO', 'SAC', 'TD3', 'A2C']\n",
        "\n",
        "processes = []\n",
        "for algorithm in algorithms:\n",
        "    p = multiprocessing.Process(target=train, args=(env, algorithm,))\n",
        "    p.start()\n",
        "    processes.append(p)\n",
        "for p in processes:\n",
        "    p.join()\n",
        "\n",
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choose of the best Algorithm and parameter tuning\n",
        "![Training Performance](./imgs/Algo_Training.JPG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "log_dir = \"SAC_logs\"\n",
        "video_dir = \"SAC_videos\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp_LdiqphilV",
        "outputId": "c6f5f1b6-3620-42a3-c841-072b3ab7b9fb"
      },
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "#model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, tensorboard_log=log_dir)\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of default Soft Actor-Critic (SAC) training**\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training. After 2,000 epochs and approx. 3 hours, 1,540,000 timesteps were performed and the reward converged:\n",
        "\n",
        "![Reward Function](./imgs/SAC_Reward_Situp.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was only possible to train stable sitting and not complete standing up:\n",
        "| Episode 0 | Episode 343 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/SAC_RL_Episode_0.gif) | ![SAC_RL_Episode_343](./imgs/SAC_RL_Episode_343.gif) |\n",
        "\n",
        "| Episode 1000 | Episode 2000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_1000](./imgs/SAC_RL_Episode_1000.gif) | ![SAC_RL_Episode_2000](./imgs/SAC_RL_Episode_2000.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "The Humanoid Standup Task is very complex and provides a huge observation and action space.\n",
        "I suspect that sitting is a saddle point that needs to be \"overcome\".\n",
        "To achieve this, the agent could be made a little more exploratory by increasing entropy or action noise.\n",
        "An alternative would be to adjust the reward function to give more weight to the upward movement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 64 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.005\n",
        "gamma = 0.99\n",
        "ent_coef=0.5 # Entropy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of default Soft Actor-Critic (SAC) training**\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training. After 2,000 epochs and approx. 3.9 hours, 2,000,000 timesteps were performed and the reward converged:\n",
        "\n",
        "![Reward Function](./imgs/SAC_Reward_Situp_param_set_1.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was only possible to train stable sitting and not complete standing up:\n",
        "| Episode 0 | Episode 343 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/SAC_RL_Episode_0_para_tuning_1.gif) | ![SAC_RL_Episode_343](./imgs/SAC_RL_Episode_343_para_tuning_1.gif) |\n",
        "\n",
        "| Episode 1000 | Episode 2000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_1000](./imgs/SAC_RL_Episode_1000_para_tuning_1.gif) | ![SAC_RL_Episode_2000](./imgs/SAC_RL_Episode_2000_para_tuning_1.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "By adding action noise and increasing the entropy, the humanoid learns new strategies for leveling up. However, even by changing these parameters, the saddle point of sitting cannot be overcome and the reward converges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./SAC_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change the Reward Function\n",
        "\n",
        "As the reward converged in the previous attempts at sitting, an attempt is now made to give greater weight to the upward movement when standing up at the start of the training.\n",
        "For this purpose, a wrapper is used to implement a separate reward function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.current_step = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # Call the step method from the Environment\n",
        "        observation, reward, done, done2, info = super().step(action)\n",
        "        \n",
        "        reward_linup = info.get('reward_linup')\n",
        "        reward_quadctrl = info.get('reward_quadctrl')\n",
        "        reward_impact = info.get('reward_impact')\n",
        "\n",
        "        # Weight the move up higher at the Beginning of the Training and then decrease\n",
        "        reward_term = max(1, 1000 - self.current_step * 0.01)\n",
        "        reward = reward_term*reward_linup + reward_quadctrl + reward_impact +1\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        return observation, reward, done, done2, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 512 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.005\n",
        "gamma = 0.99\n",
        "ent_coef=0.4 # Entropy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = CustomRewardWrapper(env)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of default Soft Actor-Critic (SAC) training**\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training. At the beginning the reward decreased due to the decreasing reward function.\n",
        "After 2,000 epochs and approx. 4 hours, 2,000,000 timesteps were performed and the reward converged at a lower level than the default reward:\n",
        "\n",
        "![Reward Function](./imgs/rl_shaping/SAC_Reward_reward_shaping_1.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was only possible to train stable sitting and not complete standing up:\n",
        "| Episode 0 | Episode 343 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/rl_shaping/SAC_RL_Episode_0_reward_shaping_1.gif) | ![SAC_RL_Episode_343](./imgs/rl_shaping/SAC_RL_Episode_343_reward_shaping_1.gif) |\n",
        "\n",
        "| Episode 1000 | Episode 2000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_1000](./imgs/rl_shaping/SAC_RL_Episode_1000_reward_shaping_1.gif) | ![SAC_RL_Episode_2000](./imgs/rl_shaping/SAC_RL_Episode_2000_reward_shaping_1.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "By changing the reward function, the agent made no real progress and it was not possible to start learning new actions after the decline phase.\n",
        "An attempt is now being made to make the decrease in rewards slower at the beginning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change the Reward Function with slower decrease\n",
        "\n",
        "Training the agent with a rapidly decreasing reward function did not improve the training. An attempt is therefore now being made to reduce the reward more slowly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.current_step = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # Call the step method from the Environment\n",
        "        observation, reward, done, done2, info = super().step(action)\n",
        "        \n",
        "        reward_linup = info.get('reward_linup')\n",
        "        reward_quadctrl = info.get('reward_quadctrl')\n",
        "        reward_impact = info.get('reward_impact')\n",
        "\n",
        "        # Weight the move up higher at the Beginning of the Training and then decrease\n",
        "        reward_term = max(1, 10 - self.current_step * 0.0001)\n",
        "        reward = reward_term*reward_linup + reward_quadctrl + reward_impact +1\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        return observation, reward, done, done2, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 256 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.005\n",
        "gamma = 0.99\n",
        "ent_coef=0.3 # Entropy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = CustomRewardWrapper(env)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of default Soft Actor-Critic (SAC) training**\n",
        "\n",
        "Due to the converging rewards, the training was stopped after 1,020,000 timesteps.\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training. At the beginning the reward decreased due to the decreasing reward function.\n",
        "The reward converged at a lower level than the default reward:\n",
        "\n",
        "![Reward Function](./imgs/rl_shaping/SAC_Reward_reward_shaping_2.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was only possible to train stable sitting and not complete standing up:\n",
        "| Episode 0 | Episode 64 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/rl_shaping/SAC_RL_Episode_0_reward_shaping_2.gif) | ![SAC_RL_Episode_343](./imgs/rl_shaping/SAC_RL_Episode_64_reward_shaping_2.gif) |\n",
        "\n",
        "| Episode 343 | Episode 1000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_1000](./imgs/rl_shaping/SAC_RL_Episode_343_reward_shaping_2.gif) | ![SAC_RL_Episode_2000](./imgs/rl_shaping/SAC_RL_Episode_1000_reward_shaping_2.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "Once again, the increased emphasis on standing up at the beginning of the training did not bring any improvement and the agent was not able to learn anything.\n",
        "We are now trying to give more weight to standing up but keep the weighting stable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change the Reward Function with a constant leverage of the upward movement\n",
        "\n",
        "Training the agent with a slower decreasing reward function did not improve the training. An attempt is therefore now being made to keep the reward constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Call the step method from the Environment\n",
        "        observation, reward, done, done2, info = super().step(action)\n",
        "        \n",
        "        reward_linup = info.get('reward_linup')\n",
        "        reward_quadctrl = info.get('reward_quadctrl')\n",
        "        reward_impact = info.get('reward_impact')\n",
        "\n",
        "        # Weight the move up higher at the Beginning of the Training and then decrease\n",
        "        reward_term = 1.1\n",
        "        reward = reward_term*reward_linup + reward_quadctrl + reward_impact +1\n",
        "\n",
        "        return observation, reward, done, done2, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 256 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.005\n",
        "gamma = 0.99\n",
        "ent_coef=0.5 # Entropy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = CustomRewardWrapper(env)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of default Soft Actor-Critic (SAC) training**\n",
        "\n",
        "Due to the decreasing reward, the training was stopped after 1,020,000 timesteps.\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training. At the beginning the reward decreased due to the decreasing reward function.\n",
        "The reward converged at a lower level than the default reward:\n",
        "\n",
        "![Reward Function](./imgs/rl_shaping/SAC_Reward_reward_shaping_3.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was only possible to train stable sitting and not complete standing up:\n",
        "| Episode 0 | Episode 64 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/rl_shaping/SAC_RL_Episode_0_reward_shaping_3.gif) | ![SAC_RL_Episode_343](./imgs/rl_shaping/SAC_RL_Episode_64_reward_shaping_3.gif) |\n",
        "\n",
        "| Episode 343 | Episode 1000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_1000](./imgs/rl_shaping/SAC_RL_Episode_343_reward_shaping_3.gif) | ![SAC_RL_Episode_2000](./imgs/rl_shaping/SAC_RL_Episode_1000_reward_shaping_3.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "With this configuration, the seated position was achieved very quickly. Unfortunately, the sitting position could not be overcome this time either.\n",
        "\n",
        "An attempt is now made to increase the constant weighting of the upward movement again and to initially set the entropy parameter high and then learn it adaptively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change the Reward Function with a higher constant leverage of the upward movement and higher entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Call the step method from the Environment\n",
        "        observation, reward, done, done2, info = super().step(action)\n",
        "        \n",
        "        reward_linup = info.get('reward_linup')\n",
        "        reward_quadctrl = info.get('reward_quadctrl')\n",
        "        reward_impact = info.get('reward_impact')\n",
        "\n",
        "        # Weight the move up higher at the Beginning of the Training and then decrease\n",
        "        reward_term = 1.7\n",
        "        reward = reward_term*reward_linup + reward_quadctrl + reward_impact +1\n",
        "\n",
        "        return observation, reward, done, done2, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 64 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.005\n",
        "gamma = 0.99\n",
        "ent_coef='auto_0.8' # Entropy, learn it automatically using start value 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = CustomRewardWrapper(env)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of the training**\n",
        "\n",
        "Due to the decreasing reward and converging in a crouching position, the training was stopped after 1,012,000 timesteps.\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training.\n",
        "The reward converged at a lower level than the default reward:\n",
        "\n",
        "![Reward Function](./imgs/rl_shaping/SAC_Reward_reward_shaping_4.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was only possible to train a crouching position and not complete standing up:\n",
        "| Episode 0 | Episode 64 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/rl_shaping/SAC_RL_Episode_0_reward_shaping_4.gif) | ![SAC_RL_Episode_64](./imgs/rl_shaping/SAC_RL_Episode_64_reward_shaping_4.gif) |\n",
        "\n",
        "| Episode 343 | Episode 1000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_343](./imgs/rl_shaping/SAC_RL_Episode_343_reward_shaping_4.gif) | ![SAC_RL_Episode_1000](./imgs/rl_shaping/SAC_RL_Episode_1000_reward_shaping_4.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "With this configuration, it was only possible to remain in a crouching position.\n",
        "\n",
        "An attempt is now being made to weight the upward movement constantly at 1.1 and to further increase the exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change the Reward Function with a constant leverage of the upward movement and high exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Call the step method from the Environment\n",
        "        observation, reward, done, done2, info = super().step(action)\n",
        "        \n",
        "        reward_linup = info.get('reward_linup')\n",
        "        reward_quadctrl = info.get('reward_quadctrl')\n",
        "        reward_impact = info.get('reward_impact')\n",
        "\n",
        "        # Weight the move up higher at the Beginning of the Training and then decrease\n",
        "        reward_term = 1.1\n",
        "        reward = reward_term*reward_linup + reward_quadctrl + reward_impact +1\n",
        "\n",
        "        return observation, reward, done, done2, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 64 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.01\n",
        "gamma = 0.99\n",
        "ent_coef='auto_0.8' # Entropy, learn it automatically using start value 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = CustomRewardWrapper(env)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, tau=tau, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parralel training with default reward, default parameters and Custom Neuronal network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 5000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1400000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 128 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.01\n",
        "gamma = 0.99\n",
        "ent_coef='auto_0.4' # Entropy, learn it automatically using start value 0.8\n",
        "\n",
        "# Define the policy kwargs with custom network architecture\n",
        "policy_kwargs = dict(\n",
        "    net_arch=[256, 256, 256]  # Increased depth of the Network\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Logging to SAC_logs/SAC_0\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 1360     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -367     |\n",
            "|    critic_loss     | 1.07e+03 |\n",
            "|    ent_coef        | 0.212    |\n",
            "|    ent_coef_loss   | 0.0988   |\n",
            "|    learning_rate   | 0.02     |\n",
            "|    n_updates       | 998      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 1360     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 88       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6e+04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 100      |\n",
            "|    fps             | 1359     |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -723     |\n",
            "|    critic_loss     | 1.36e+03 |\n",
            "|    ent_coef        | 0.413    |\n",
            "|    ent_coef_loss   | -0.922   |\n",
            "|    learning_rate   | 0.02     |\n",
            "|    n_updates       | 1998     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 108      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 116      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 120      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 128      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 136      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 140      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 144      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 148      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 152      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 156      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 160      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 164      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 168      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 172      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 176      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 180      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 184      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 188      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 192      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 196      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 200      |\n",
            "|    fps             | 1201     |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | 6.25e+04  |\n",
            "| time/              |           |\n",
            "|    episodes        | 204       |\n",
            "|    fps             | 1177      |\n",
            "|    time_elapsed    | 254       |\n",
            "|    total_timesteps | 300000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.19e+03 |\n",
            "|    critic_loss     | 983       |\n",
            "|    ent_coef        | 0.674     |\n",
            "|    ent_coef_loss   | 0.172     |\n",
            "|    learning_rate   | 0.02      |\n",
            "|    n_updates       | 2998      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 208      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 212      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 216      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 220      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 224      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 228      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 232      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 236      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 240      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 244      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 248      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 252      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 256      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 260      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 264      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 268      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 272      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 276      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 280      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 284      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 288      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 292      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 296      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.25e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 300      |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m SAC(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, \n\u001b[1;32m     15\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size, tau\u001b[38;5;241m=\u001b[39mtau, gamma\u001b[38;5;241m=\u001b[39mgamma, action_noise\u001b[38;5;241m=\u001b[39maction_noise, \n\u001b[1;32m     16\u001b[0m             ent_coef\u001b[38;5;241m=\u001b[39ment_coef, tensorboard_log\u001b[38;5;241m=\u001b[39mlog_dir, policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Save the agent\u001b[39;00m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msac_humanoid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/DeepRL/.venv/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DeepRL/.venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[0;32m~/DeepRL/.venv/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:268\u001b[0m, in \u001b[0;36mSAC.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    267\u001b[0m critic_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Alternative: actor_loss = th.mean(log_prob - qf1_pi)\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Min over all critic networks\u001b[39;00m\n\u001b[1;32m    273\u001b[0m q_values_pi \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(replay_data\u001b[38;5;241m.\u001b[39mobservations, actions_pi), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/DeepRL/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/DeepRL/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m~/DeepRL/.venv/lib/python3.10/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/DeepRL/.venv/lib/python3.10/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DeepRL/.venv/lib/python3.10/site-packages/torch/optim/adam.py:525\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    522\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m    524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_addcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# Delete the local intermediate since it won't be used anymore to save on peak memory\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m device_grads\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Create and wrap the environment\n",
        "num_envs = 50\n",
        "env = make_vec_env('HumanoidStandup-v4', n_envs=num_envs, env_kwargs={\"render_mode\": \"rgb_array\", \"max_episode_steps\": 1000})\n",
        "\n",
        "# Wrap the environment to record videos\n",
        "env = VecVideoRecorder(env, video_dir, record_video_trigger=lambda x: x % 2000 == 0, video_length=1000)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "# Initialize the SAC model with the custom network architecture\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, \n",
        "            batch_size=batch_size, tau=tau, gamma=gamma, action_noise=action_noise, \n",
        "            ent_coef=ent_coef, tensorboard_log=log_dir, policy_kwargs=policy_kwargs)\n",
        "\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "# Delete the trained model to demonstrate loading\n",
        "del model\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=vec_env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
        "\n",
        "# Enjoy the trained agent\n",
        "obs = vec_env.reset()\n",
        "for _ in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")\n",
        "\n",
        "# Close the video recorder\n",
        "vec_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
