{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWZ5WpWmhAE9"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
        "    <img style=\"float: right;\" src=\"imgs/OST.png\" width=260, height=130>\n",
        "</div>\n",
        "<div style=\"text-align: center;\">\n",
        "    <h1>Learning a Humanoid to Standup with Reinforcement Learning</h1>\n",
        "    <h2>Christoph Landolt</h2>\n",
        "    <h3>20. June 2024</h3>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n",
        "\n",
        "**Task Description**\n",
        "\n",
        "In this Notebook, the HumanoidStandup Task is solved. This task involves training a reinforcement learning agent to control a humanoid model consisting of 15 body parts to transition from a horizontal lying position to a standing posture.\n",
        "This task is especially challenging because the agent interacts with a high-dimensional action space of 17 actions corresponding to joint angles and velocities and observes a state space of 376 dimensions, which includes information about the Humanoid's position, velocity, and angular velocity.\n",
        "The task is considered solved when the Humanoid successfully stands up without falling over. This demonstrates the agent's ability to handle complex, high-dimensional control tasks.\n",
        "\n",
        "**Project Scope**\n",
        "\n",
        "The aim of this project is to evaluate the best reinforcement learning algorithm for the HumanoidStandup task and then to optimize the parameters of this algorithm.\n",
        "Subsequently, reward shaping will be carried out and it will be shown how several agents can be trained in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Objective and approach\n",
        "\n",
        "**Objective**\n",
        "This project aims to train a horizontal-lying humanoid robot, simulated in the Mujoco physics engine, to stand up independently.\n",
        "\n",
        "**Approach**\n",
        "\n",
        "The project is carried out in the following steps:\n",
        "1) Comparison of the following RL algorithms for this task: PPO, SAC, TD3 and A2C with the default parameters by comparing the reward. The choice of the RL algorithm will be verified with the current literature.\n",
        "2) Tuning the parameters of the selected RL algorithm\n",
        "3) Creating and registering an environment with adjusted reward\n",
        "4) Parallel training of the agent\n",
        "5) Discussion of the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Python Library Requirements\n",
        "\n",
        "This project requires the following Python libraries:\n",
        "\n",
        "- mujoco\n",
        "- gymnasium\n",
        "- stable-baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Documentation of the Mujoco Humanoid simulation (Adapted from [Gymnasium Documentation](https://gymnasium.farama.org/environments/mujoco/humanoid_standup/))\n",
        "The environment which is described in [Paper](https://ieeexplore.ieee.org/document/6386025) simulates a human consisting of two legs, two arms and a Torso.\n",
        "\n",
        "The environment is designet to learn a humanoid to stand up or to lern how to walk.\n",
        "\n",
        "### Overview\n",
        "| Description | Information |\n",
        "|----------|----------|\n",
        "| Action Space   | ``` Box(-0.4, 0.4, (17,), float32) ```   |\n",
        "| Observation Space    | ```Box(-inf, inf, (348,), float64)```     |\n",
        "| import learning to Walk    | \t```gymnasium.make(\"Humanoid-v4\")```     |\n",
        "| import standup   | ```gymnasium.make(\"HumanoidStandup-v4\")```     |\n",
        "\n",
        "\n",
        "### Action Space\n",
        "An action represents the torques applied at the hinge joints.\n",
        "\n",
        "![imgs/humanoid.png](imgs/humanoid.png)\n",
        "\n",
        "| Num | Name  | Joint | Type (Unit) | \n",
        "|----------|----------|----------|----------|\n",
        "| 0 | abdomen_y | hinge | torque (N m) |\n",
        "| 1 | abdomen_z | hinge | torque (N m) | \n",
        "| 2 | abdomen_x | hinge | torque (N m) |\n",
        "| 3 | right_hip_x | hinge | torque (N m) |\n",
        "| 4 | right_hip_z | hinge | torque (N m) |\n",
        "| 5 | right_hip_y | hinge | torque (N m) |\n",
        "| 6 | right_knee | hinge | torque (N m) | \n",
        "| 7 | left_hip_x  | hinge | torque (N m) |\n",
        "| 8 | left_hip_z | hinge | torque (N m) |\n",
        "| 9 | left_hip_y | hinge | torque (N m) | \n",
        "| 10 | left_knee | hinge | torque (N m) | \n",
        "| 11 | right_shoulder1 | hinge | torque (N m) | \n",
        "| 12 | right_shoulder2 | hinge | torque (N m) | \n",
        "| 13 | right_elbow | hinge | torque (N m) | \n",
        "| 14 | left_shoulder1 | hinge | torque (N m) | \n",
        "| 15 | left_shoulder2 | hinge | torque (N m) | \n",
        "| 16 | left_elbow | hinge | torque (N m) | \n",
        "\n",
        "### Observation Space\n",
        "The observation space for the Gymnasium HumanoidStandup task is a 376-dimensional continuous space. This high-dimensional space captures a wide range of information about the state of the humanoid model, which consists of 15 body parts.\n",
        "\n",
        "The state vector includes the following information:\n",
        "\n",
        "- Position and velocity of the humanoid's center of mass\n",
        "- Position, velocity, and angular velocity of each of the humanoid's body parts\n",
        "- Joint angles and velocities\n",
        "- Contact forces\n",
        "- Boolean indicators for whether each body part is in contact with the ground\n",
        "\n",
        "This rich state representation allows the reinforcement learning agent to have a comprehensive understanding of the current state of the humanoid and its environment, which is crucial for learning to balance and coordinate the humanoid's movements to stand up from a prone position.\n",
        "\n",
        "A detailed overview can be found under this [Link](https://gymnasium.farama.org/environments/mujoco/humanoid_standup/).\n",
        "\n",
        "### Rewards\n",
        "**Standup-Task**\n",
        "\n",
        "The total reward is: reward = uph_cost + 1 - quad_ctrl_cost - quad_impact_cost.\n",
        "- uph_cost: A reward for moving up\n",
        "- quad_ctrl_cost: A negative reward to penalize the Humanoid for taking actions that are too large.\n",
        "- impact_cost: A negative reward to penalize the Humanoid if the external contact forces are too large.\n",
        "\n",
        "A detailed overview can be found under this [Link](https://gymnasium.farama.org/environments/mujoco/humanoid_standup/).\n",
        "\n",
        "**Learning to Walk-Task**\n",
        "\n",
        "The reward function for the Humanoid learning to walk task is composed of four parts:\n",
        "\n",
        "1. Healthy Reward: A fixed reward given at every timestep that the humanoid is alive.\n",
        "\n",
        "2. Forward Reward: A reward for moving forward, calculated as `forward_reward_weight * (average center of mass before action - average center of mass after action) / dt`. Here, `dt` is the time between actions, dependent on the `frame_skip` parameter.\n",
        "\n",
        "3. Control Cost: A penalty for large control forces, calculated as `ctrl_cost_weight * sum(control^2)`.\n",
        "\n",
        "4. Contact Cost: A penalty for large external contact forces.\n",
        "\n",
        "The total reward is calculated as `reward = healthy_reward + forward_reward - ctrl_cost - contact_cost`.\n",
        "\n",
        "A detailed overview can be found under this [Link](https://gymnasium.farama.org/environments/mujoco/humanoid/#)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Description of the used Algorithms\n",
        "The following 4 agents are being evaluated for this project:\n",
        "\n",
        "### 5.1 Soft Actor-Critic (SAC) (Theory adapted from [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/sac.html#id2) implemented with [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html))\n",
        "The Soft Actor-Critic was introduced in the following Paper: [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290)\n",
        "\n",
        "The Algorithm is more sample efficient than A2C or PPO because it is off-policy and can reuse data stored in a experience replay memory.\n",
        "The SAC Algorithm is also a maximum-Entropy RL-framework and by combining the sample efficency with the Entropy the SAC Algorithm is more stable in training. (Fundations of Deep Reinforcement Learning (978-0-13-517238-4))\n",
        "\n",
        "The objective function for SAC is:\n",
        "\n",
        "$$J(\\pi) = \\mathbb{E}\\left[\\sum_{t} \\gamma^{t} \\left(r(s_t, a_t) + \\alpha H(\\pi(\\cdot|s_t))\\right)\\right]$$\n",
        "\n",
        "where $r(s_t, a_t)$ is the reward, $H(\\pi(\\cdot|s_t))$ is the entropy of the policy $\\pi$ at state $s_t$, $\\gamma$ is the discount factor, and $\\alpha$ is the temperature parameter that determines the trade-off between exploration and exploitation.\n",
        "\n",
        "The policy update rule in SAC is:\n",
        "\n",
        "$$\\pi^{*} = \\arg\\min_{\\pi} D_{KL}\\left(\\pi(\\cdot|s_t) || \\exp\\left(\\frac{Q(s_t, \\cdot)}{Z^{\\pi_{old}(s_t)}}\\right)\\right)$$\n",
        "\n",
        "where $Z^{\\pi_{old}(s_t)}$ is the partition function to normalizes the distribution.\n",
        "\n",
        "The SAC algorithm uses two Q-functions to reduce overestimation bias in value estimation by taking the minimum of the two Q-values during the policy update step.\n",
        "\n",
        "SAC has been shown to achieve state-of-the-art performance on a range of continuous control tasks due to its stability and efficiency.\n",
        "\n",
        "\n",
        "### 5.2 Proximal Policy Optimization (PPO) (Theory adapted from [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/ppo.html) implemented with [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html))\n",
        "The Proximal Policy Optimization Algorithms was introduced in the following Paper: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n",
        "\n",
        "The goal of the Proximal Policy Optimization (PPO) algorithm is to optimize a policy by making small updates that improve performance while staying close to the current policy, thereby maintaining stability and avoiding harmful large updates.\n",
        "\n",
        "The objective function in PPO is:\n",
        "\n",
        "$$L^{CLIP}(\\theta) = \\hat{E}_t[min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$\n",
        "\n",
        "where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio of the new and old policies, $\\hat{A}_t$ is the estimated advantage at time $t$, and $\\epsilon$ is a hyperparameter that controls the degree of policy change.\n",
        "\n",
        "### 5.3 Advantage Actor Critic (A2C) (Theory adapted from [Hugging Face](https://huggingface.co/blog/deep-rl-a2c) implemented with [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html))\n",
        "\n",
        "The Advantage Actor Critic Algorithms was introduced in the following Paper: [Asynchronous Methods for Deep Reinforcement Learning\n",
        "](https://arxiv.org/abs/1602.01783)\n",
        "\n",
        "The goal of the Advantage Actor-Critic (A2C) algorithm is to optimize a policy by using an actor to choose actions and a critic to estimate the value function, thereby balancing exploration and exploitation while learning from the difference between the actual and expected rewards (the advantage).\n",
        "\n",
        "The actor is the policy that the agent uses to interact with the environment, while the critic estimates the value function to help the actor improve its policy. The advantage function, defined as $A(s, a) = Q(s, a) - V(s)$, measures how much better an action is compared to the average action at that state.\n",
        "\n",
        "The objective function in A2C is:\n",
        "\n",
        "$$L(\\theta) = \\hat{E}_t[\\log \\pi_\\theta(a_t|s_t) \\hat{A}_t]$$\n",
        "\n",
        "where $\\pi_\\theta(a_t|s_t)$ is the policy (actor), $\\hat{A}_t$ is the estimated advantage at time $t$, and $\\theta$ are the parameters of the policy.\n",
        "\n",
        "A2C uses synchronous updates for efficiency and stability. It is an online algorithm, learning while the agent interacts with the environment.\n",
        "\n",
        "### 5.4 TD3: Twin Delayed DDPG (Deep Deterministic Policy Gradient) (Theory adapted from [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/td3.html#id1) implemented with [Stable-Baselines3](https://stable-baselines.readthedocs.io/en/master/modules/td3.html?highlight=TD3))\n",
        "\n",
        "The Twin Delayed DDPG Algorithms was introduced in the following Paper: [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/pdf/1802.09477)\n",
        "\n",
        "Twin Delayed Deep Deterministic Policy Gradient (TD3) is an algorithm for reinforcement learning that improves upon the Deep Deterministic Policy Gradient (DDPG) method. TD3 addresses the overestimation bias in Q-value estimation by maintaining two Q-functions and using the smaller of the two Q-values to update the policy. It also introduces a delay in the policy update to reduce the variance of policy and Q-value updates.\n",
        "\n",
        "The two Q-functions, $Q_{\\theta_1}$ and $Q_{\\theta_2}$, are used to reduce overestimation bias. The Q-functions are updated by minimizing the following loss functions:\n",
        "\n",
        "$$L(\\theta_1) = \\mathbb{E}_{s,a,r,s'\\sim D}\\left[\\left(Q_{\\theta_1}(s,a) - (r + \\gamma \\min_{j=1,2} Q_{\\theta_{j_{old}}}(s',\\pi_{\\phi_{old}}(s')))\\right)^2\\right]$$\n",
        "\n",
        "$$L(\\theta_2) = \\mathbb{E}_{s,a,r,s'\\sim D}\\left[\\left(Q_{\\theta_2}(s,a) - (r + \\gamma \\min_{j=1,2} Q_{\\theta_{j_{old}}}(s',\\pi_{\\phi_{old}}(s')))\\right)^2\\right]$$\n",
        "\n",
        "where $Q_{\\theta_1}(s,a)$ and $Q_{\\theta_2}(s,a)$ are the two Q-functions, $r$ is the reward, $\\gamma$ is the discount factor, $\\pi_{\\phi_{old}}(s')$ is the old policy, and $\\theta_{j_{old}}$ are the old parameters of the Q-functions.\n",
        "\n",
        " the policy is updated by minimizing the following loss function:\n",
        "\n",
        "$$\\pi^{*} = \\max_{\\theta} \\mathbb{E}_{s\\sim D}[Q_{\\theta_1}(s, \\pi_\\phi(s))]$$\n",
        "\n",
        "TD3 is an online algorithm, learning while the agent interacts with the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiPrsLZShX3t",
        "outputId": "d6243f84-df22-45c7-e829-1656409a14a9"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "from stable_baselines3 import SAC, TD3, A2C, PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choice of the best RL Algorithm\n",
        "1. The basic idea is to test several RL algorithms with the standard parameters in parallel and to monitor the training progress using the reward.\n",
        "2. Subsequently, the algorithm that makes the fastest training progress is to be implemented and tuned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directories to hold models and logs for the tensor board\n",
        "model_dir = \"models\"\n",
        "log_dir = \"logs\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(log_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define training parameters\n",
        "TIMESTEPS = 25000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(env, humanoid_training_algo):\n",
        "    match humanoid_training_algo:\n",
        "        case 'PPO':\n",
        "            model = PPO('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case 'SAC':\n",
        "            model = SAC('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case 'TD3':\n",
        "            model = TD3('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case 'A2C':\n",
        "            model = A2C('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case _:\n",
        "            print('Algorithm not found')\n",
        "            return\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "\n",
        "        model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False)\n",
        "        model.save(f\"{model_dir}/{humanoid_training_algo}_{TIMESTEPS*i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train the Model**\n",
        "After one Algorithm performs clearly better than the others interrupt the training in the Jupyter Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"None\")\n",
        "algorithms = ['PPO', 'SAC', 'TD3', 'A2C']\n",
        "\n",
        "processes = []\n",
        "for algorithm in algorithms:\n",
        "    p = multiprocessing.Process(target=train, args=(env, algorithm,))\n",
        "    p.start()\n",
        "    processes.append(p)\n",
        "for p in processes:\n",
        "    p.join()\n",
        "\n",
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choose of the best Algorithm and parameter tuning\n",
        "![Training Performance](./imgs/Algo_Training.JPG)\n",
        "\n",
        "The comparison of the four algorithms A2C, PPO, SAC and TD3 shows that the algorithms A2C, PPO and TD3 converge very early on with a low reward, while the SAC achieves a much higher reward and continues to learn.\n",
        "\n",
        "This is also consistent with the literature:\n",
        "- [ATTRACTION-REPULSION ACTOR-CRITIC FOR CONTINUOUS CONTROL REINFORCEMENT LEARNING](https://arxiv.org/pdf/1909.07543)\n",
        "- [Evolving Rewards to Automate Reinforcement Learning](https://arxiv.org/pdf/1905.07628)\n",
        "- Fundations of Deep Reinforcement Learning (978-0-13-517238-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic training of the SAC algorithm with default parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "log_dir = \"SAC_logs\"\n",
        "video_dir = \"SAC_videos\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp_LdiqphilV",
        "outputId": "c6f5f1b6-3620-42a3-c841-072b3ab7b9fb"
      },
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "#model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, tensorboard_log=log_dir)\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of default Soft Actor-Critic (SAC) training**\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training. After 2,000 epochs and approx. 3 hours, 1,540,000 timesteps were performed and the reward converged:\n",
        "\n",
        "![Reward Function](./imgs/SAC_Reward_Situp.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was only possible to train stable sitting and not complete standing up:\n",
        "| Episode 0 | Episode 343 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/SAC_RL_Episode_0.gif) | ![SAC_RL_Episode_343](./imgs/SAC_RL_Episode_343.gif) |\n",
        "\n",
        "| Episode 1000 | Episode 2000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_1000](./imgs/SAC_RL_Episode_1000.gif) | ![SAC_RL_Episode_2000](./imgs/SAC_RL_Episode_2000.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "The Humanoid Standup Task is very complex and provides a huge observation and action space.\n",
        "I suspect that sitting is a saddle point that needs to be \"overcome\".\n",
        "To achieve this, the agent could be made a little more exploratory by increasing entropy or action noise.\n",
        "An alternative would be to adjust the reward function to give more weight to the upward movement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./SAC_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parameter tuning\n",
        "\n",
        "Change the following parameters:\n",
        "- Set the buffer_size to 1000000 (no change to the default parameter)\n",
        "- Increase the learning rate\n",
        "- Decrease the batch_size\n",
        "- Do not change the default discount factor\n",
        "- Set the entropy to 0.5 from auto\n",
        "- Add NormalActionNoise \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 64 # number of experiences sampled from the replay buffer for each update of the model\n",
        "gamma = 0.99 # discount factor\n",
        "ent_coef=0.5 # Entropy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of the training**\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training. After 2,000 epochs and approx. 3.9 hours, 2,000,000 timesteps were performed and the reward converged:\n",
        "\n",
        "![Reward Function](./imgs/SAC_Reward_Situp_param_set_1.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was only possible to train stable sitting and not complete standing up:\n",
        "| Episode 0 | Episode 343 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/SAC_RL_Episode_0_para_tuning_1.gif) | ![SAC_RL_Episode_343](./imgs/SAC_RL_Episode_343_para_tuning_1.gif) |\n",
        "\n",
        "| Episode 1000 | Episode 2000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_1000](./imgs/SAC_RL_Episode_1000_para_tuning_1.gif) | ![SAC_RL_Episode_2000](./imgs/SAC_RL_Episode_2000_para_tuning_1.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "By adding action noise and increasing the entropy, the humanoid learns new strategies for leveling up. However, even by changing these parameters, the saddle point of sitting cannot be overcome and the reward converges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./SAC_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change the Reward Function\n",
        "\n",
        "As the reward converged in the previous attempts at sitting, an attempt is now made to give greater weight to the upward movement when standing up at the start of the training.\n",
        "For this purpose, a wrapper is used to implement a separate reward function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.current_step = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # Call the step method from the Environment\n",
        "        observation, reward, done, done2, info = super().step(action)\n",
        "        \n",
        "        reward_linup = info.get('reward_linup')\n",
        "        reward_quadctrl = info.get('reward_quadctrl')\n",
        "        reward_impact = info.get('reward_impact')\n",
        "\n",
        "        # Weight the move up higher at the Beginning of the Training and then decrease\n",
        "        reward_term = max(1, 1000 - self.current_step * 0.01)\n",
        "        reward = reward_term*reward_linup + reward_quadctrl + reward_impact +1\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        return observation, reward, done, done2, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 512 # number of experiences sampled from the replay buffer for each update of the model\n",
        "gamma = 0.99 # discount factor\n",
        "ent_coef=0.4 # Entropy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = CustomRewardWrapper(env)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of the training**\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training. At the beginning the reward decreased due to the decreasing reward function.\n",
        "After 2,000 epochs and approx. 4 hours, 2,000,000 timesteps were performed and the reward converged at a lower level than the default reward:\n",
        "\n",
        "![Reward Function](./imgs/rl_shaping/SAC_Reward_reward_shaping_1.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was not possible to train stable sitting and not complete standing up:\n",
        "| Episode 0 | Episode 343 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/rl_shaping/SAC_RL_Episode_0_reward_shaping_1.gif) | ![SAC_RL_Episode_343](./imgs/rl_shaping/SAC_RL_Episode_343_reward_shaping_1.gif) |\n",
        "\n",
        "| Episode 1000 | Episode 2000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_1000](./imgs/rl_shaping/SAC_RL_Episode_1000_reward_shaping_1.gif) | ![SAC_RL_Episode_2000](./imgs/rl_shaping/SAC_RL_Episode_2000_reward_shaping_1.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "By changing the reward function, the agent made no real progress and it was not possible to start learning new actions after the decline phase.\n",
        "An attempt is now being made to make the decrease in rewards slower at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./SAC_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change the Reward Function with slower decrease\n",
        "\n",
        "Training the agent with a rapidly decreasing reward function did not improve the training. An attempt is therefore now being made to reduce the reward more slowly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.current_step = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # Call the step method from the Environment\n",
        "        observation, reward, done, done2, info = super().step(action)\n",
        "        \n",
        "        reward_linup = info.get('reward_linup')\n",
        "        reward_quadctrl = info.get('reward_quadctrl')\n",
        "        reward_impact = info.get('reward_impact')\n",
        "\n",
        "        # Weight the move up higher at the Beginning of the Training and then decrease\n",
        "        reward_term = max(1, 10 - self.current_step * 0.0001)\n",
        "        reward = reward_term*reward_linup + reward_quadctrl + reward_impact +1\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        return observation, reward, done, done2, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 256 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.005 # target smoothing coefficient\n",
        "gamma = 0.99 # discount factor\n",
        "ent_coef=0.3 # Entropy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = CustomRewardWrapper(env)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of the training**\n",
        "\n",
        "Due to the converging rewards, the training was stopped after 1,020,000 timesteps.\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training. At the beginning the reward decreased due to the decreasing reward function.\n",
        "The reward converged at a lower level than the default reward:\n",
        "\n",
        "![Reward Function](./imgs/rl_shaping/SAC_Reward_reward_shaping_2.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was not possible to train stable sitting and not complete standing up:\n",
        "| Episode 0 | Episode 64 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/rl_shaping/SAC_RL_Episode_0_reward_shaping_2.gif) | ![SAC_RL_Episode_343](./imgs/rl_shaping/SAC_RL_Episode_64_reward_shaping_2.gif) |\n",
        "\n",
        "| Episode 343 | Episode 1000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_1000](./imgs/rl_shaping/SAC_RL_Episode_343_reward_shaping_2.gif) | ![SAC_RL_Episode_2000](./imgs/rl_shaping/SAC_RL_Episode_1000_reward_shaping_2.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "Once again, the increased emphasis on standing up at the beginning of the training did not bring any improvement and the agent was not able to learn anything.\n",
        "We are now trying to give more weight to standing up but keep the weighting stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./SAC_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change the Reward Function with a constant leverage of the upward movement\n",
        "\n",
        "Training the agent with a slower decreasing reward function did not improve the training. An attempt is therefore now being made to keep the reward constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Call the step method from the Environment\n",
        "        observation, reward, done, done2, info = super().step(action)\n",
        "        \n",
        "        reward_linup = info.get('reward_linup')\n",
        "        reward_quadctrl = info.get('reward_quadctrl')\n",
        "        reward_impact = info.get('reward_impact')\n",
        "\n",
        "        # Weight the move up higher at the Beginning of the Training and then decrease\n",
        "        reward_term = 1.1\n",
        "        reward = reward_term*reward_linup + reward_quadctrl + reward_impact +1\n",
        "\n",
        "        return observation, reward, done, done2, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 256 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.005 # target smoothing coefficient\n",
        "gamma = 0.99 # discount factor\n",
        "ent_coef=0.5 # Entropy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = CustomRewardWrapper(env)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of the training**\n",
        "\n",
        "Due to the decreasing reward, the training was stopped after 1,020,000 timesteps.\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training. At the beginning the reward decreased due to the decreasing reward function.\n",
        "The reward converged at a lower level than the default reward:\n",
        "\n",
        "![Reward Function](./imgs/rl_shaping/SAC_Reward_reward_shaping_3.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was only possible to train stable sitting and not complete standing up:\n",
        "| Episode 0 | Episode 64 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/rl_shaping/SAC_RL_Episode_0_reward_shaping_3.gif) | ![SAC_RL_Episode_343](./imgs/rl_shaping/SAC_RL_Episode_64_reward_shaping_3.gif) |\n",
        "\n",
        "| Episode 343 | Episode 1000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_1000](./imgs/rl_shaping/SAC_RL_Episode_343_reward_shaping_3.gif) | ![SAC_RL_Episode_2000](./imgs/rl_shaping/SAC_RL_Episode_1000_reward_shaping_3.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "With this configuration, the seated position was achieved very quickly. Unfortunately, the sitting position could not be overcome this time either.\n",
        "\n",
        "An attempt is now made to increase the constant weighting of the upward movement again and to initially set the entropy parameter high and then learn it adaptively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./SAC_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change the Reward Function with a higher constant leverage of the upward movement and higher entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Call the step method from the Environment\n",
        "        observation, reward, done, done2, info = super().step(action)\n",
        "        \n",
        "        reward_linup = info.get('reward_linup')\n",
        "        reward_quadctrl = info.get('reward_quadctrl')\n",
        "        reward_impact = info.get('reward_impact')\n",
        "\n",
        "        # Weight the move up higher at the Beginning of the Training and then decrease\n",
        "        reward_term = 1.7\n",
        "        reward = reward_term*reward_linup + reward_quadctrl + reward_impact +1\n",
        "\n",
        "        return observation, reward, done, done2, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 64 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.005 # target smoothing coefficient\n",
        "gamma = 0.99   # discount factor\n",
        "ent_coef='auto_0.8' # Entropy, learn it automatically using start value 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = CustomRewardWrapper(env)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of the training**\n",
        "\n",
        "Due to the converging reward and converging in a crouching position, the training was stopped after 1,012,000 timesteps.\n",
        "\n",
        "An NVIDIA RTX 6000 GPU was used for the training.\n",
        "The reward converged at a lower level than the default reward:\n",
        "\n",
        "![Reward Function](./imgs/rl_shaping/SAC_Reward_reward_shaping_4.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was only possible to train a crouching position and not complete standing up:\n",
        "| Episode 0 | Episode 64 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/rl_shaping/SAC_RL_Episode_0_reward_shaping_4.gif) | ![SAC_RL_Episode_64](./imgs/rl_shaping/SAC_RL_Episode_64_reward_shaping_4.gif) |\n",
        "\n",
        "| Episode 343 | Episode 1000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_343](./imgs/rl_shaping/SAC_RL_Episode_343_reward_shaping_4.gif) | ![SAC_RL_Episode_1000](./imgs/rl_shaping/SAC_RL_Episode_1000_reward_shaping_4.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "With this configuration, it was only possible to remain in a crouching position.\n",
        "\n",
        "An attempt is now being made to weight the upward movement constantly at 1.1 and to further increase the exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./SAC_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change the Reward Function with a constant leverage of the upward movement and high exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Call the step method from the Environment\n",
        "        observation, reward, done, done2, info = super().step(action)\n",
        "        \n",
        "        reward_linup = info.get('reward_linup')\n",
        "        reward_quadctrl = info.get('reward_quadctrl')\n",
        "        reward_impact = info.get('reward_impact')\n",
        "\n",
        "        # Weight the move up higher at the Beginning of the Training and then decrease\n",
        "        reward_term = 1.1\n",
        "        reward = reward_term*reward_linup + reward_quadctrl + reward_impact +1\n",
        "\n",
        "        return observation, reward, done, done2, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 2000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1000000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 64 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.01 # target smoothing coefficient\n",
        "gamma = 0.99  # discount factor\n",
        "ent_coef='auto_0.8' # Entropy, learn it automatically using start value 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\", max_episode_steps=1_000)\n",
        "env = CustomRewardWrapper(env)\n",
        "env = gym.wrappers.RecordVideo(env, video_dir)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
        "\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, batch_size=batch_size, tau=tau, gamma=gamma, action_noise=action_noise, ent_coef=ent_coef, tensorboard_log=log_dir)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of the training**\n",
        "\n",
        "Even with this configuration, the humanoid converged in a sitting position after 144,000 timesteps:\n",
        "\n",
        "![Reward Function](./imgs/rl_shaping/SAC_Reward_reward_shaping_5.JPG)\n",
        "\n",
        "Unfortunately, with this model and these parameters, it was only possible to train a sitting position and not complete standing up:\n",
        "| Episode 0 | Episode 8 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_0](./imgs/rl_shaping/SAC_RL_Episode_0_reward_shaping_5.gif) | ![SAC_RL_Episode_8](./imgs/rl_shaping/SAC_RL_Episode_8_reward_shaping_5.gif) |\n",
        "\n",
        "| Episode 64 | Episode 1000 |\n",
        "|:---:|:---:|\n",
        "| ![SAC_RL_Episode_64](./imgs/rl_shaping/SAC_RL_Episode_64_reward_shaping_5.gif) | ![SAC_RL_Episode_125](./imgs/rl_shaping/SAC_RL_Episode_125_reward_shaping_5.gif) |\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "Despite the higher entropy and adjustment of the distribution of the action noise, the \"saddle point\" seating position could not be overcome.\n",
        "An attempt is now being made to further increase the entropy and train several agents in parallel so that the reward is less sparse and more data is available for training the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./SAC_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parralel training with default reward, default parameters and Custom Neuronal network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_episodes = 5000\n",
        "max_timesteps_per_episode = 1000\n",
        "\n",
        "# Calculate total timesteps\n",
        "total_timesteps = total_episodes * max_timesteps_per_episode\n",
        "\n",
        "buffer_size = 1400000 # replay buffer size\n",
        "learning_rate = 0.02 # How much to change the model due to the eccor at each iteration\n",
        "batch_size = 128 # number of experiences sampled from the replay buffer for each update of the model\n",
        "tau=0.01\n",
        "gamma = 0.99\n",
        "ent_coef='auto_0.4' # Entropy, learn it automatically using start value 0.8\n",
        "\n",
        "# Define the policy kwargs with custom network architecture\n",
        "policy_kwargs = dict(\n",
        "    net_arch=[256, 256, 256]  # Increased depth of the Network\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "num_envs = 50\n",
        "env = make_vec_env('HumanoidStandup-v4', n_envs=num_envs, env_kwargs={\"render_mode\": \"rgb_array\", \"max_episode_steps\": 1000})\n",
        "\n",
        "# Wrap the environment to record videos\n",
        "env = VecVideoRecorder(env, video_dir, record_video_trigger=lambda x: x % 2000 == 0, video_length=1000)\n",
        "\n",
        "# Define the action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "# Initialize the SAC model with the custom network architecture\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=buffer_size, learning_rate=learning_rate, \n",
        "            batch_size=batch_size, tau=tau, gamma=gamma, action_noise=action_noise, \n",
        "            ent_coef=ent_coef, tensorboard_log=log_dir, policy_kwargs=policy_kwargs)\n",
        "\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "\n",
        "# Save the agent\n",
        "model.save(\"sac_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "# Delete the trained model to demonstrate loading\n",
        "del model\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(\"sac_humanoid\", env=vec_env)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
        "\n",
        "# Enjoy the trained agent\n",
        "obs = vec_env.reset()\n",
        "for _ in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")\n",
        "\n",
        "# Close the video recorder\n",
        "vec_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Results of the training**\n",
        "\n",
        "Even with this configuration, the humanoid converged in a sitting position after 144,000 timesteps:\n",
        "\n",
        "![Reward Function](./imgs/rl_shaping/SAC_Reward_reward_shaping_6.JPG)\n",
        "\n",
        "**Step 0 to 1000**\n",
        "\n",
        "![Step 0 to 1000](./imgs/rl_shaping/SAC_parallel-step-0-to-step-1000.gif)\n",
        "\n",
        "**Step 4000 to 5000**\n",
        "\n",
        "![Step 4000 to 5000](./imgs/rl_shaping/SAC_parallel-step-4000-to-step-5000.gif)\n",
        "\n",
        "\n",
        "\n",
        "**Interpretation of the result:**\n",
        "\n",
        "Even with the high entropy, the deeper neural network and with 100 parallel environments, the task of standing up completely could not be solved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./SAC_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusion and Possible Improvements\n",
        "\n",
        "Although the complete task of standing up could not be achieved, by repeatedly achieving the intermediate goal of \"sitting down\" it could be shown that modern reinforcement learning algorithms can be used to achieve control tasks even in very complex environments.\n",
        "I would like to emphasize the following observations:\n",
        "- Choosing the right reinforcement learning algorithm for the problem plays an important role. This could be shown in a direct comparison of the algorithms. The choice of the SAC algorithm for the humanoid control task is also recommended by the authors of the book Fundations of Deep Reinforcement Learning (978-0-13-517238-4) alongside TD3 as state of the art.\n",
        "- PPO is now used as the standard algorithm in many modern RL applications. Therefore, an additional attempt was made to solve the problem using PPO. However, as can be assumed from the direct comparison, the PPO algorithm had considerable problems with a declaining reward in the humanoid control taks and was unable to solve the task as well as the SAC algorithm:\n",
        "\n",
        "![PPO Reward](./imgs/PPO_Reward.png)\n",
        "\n",
        "- Adjusting the reward function can lead to quite a few problems. However, it could be shown that with a weighting of the upward movement of 1.1, the goal of sitting could be achieved the fastest.\n",
        "\n",
        "**The following could be tried in another project:**\n",
        "- Longer training of the agent over several days or weeks\n",
        "- Changing the starting position so that you do not always start lying down but also in other positions.\n",
        "- Another attempt to adjust the reward function to favor certain movements for standing up."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
