{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWZ5WpWmhAE9"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
        "    <img style=\"float: right;\" src=\"imgs/ost.png\" width=260, height=130>\n",
        "</div>\n",
        "<div style=\"text-align: center;\">\n",
        "    <h1>Learning a Humanoid to Standup with Reinforcement Learning</h1>\n",
        "    <h2>Christoph Landolt</h2>\n",
        "    <h3>June 2024</h3>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Python Library Requirements\n",
        "\n",
        "This project requires the following Python libraries:\n",
        "\n",
        "- mujoco\n",
        "- gymnasium\n",
        "- stable-baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Documentation of the Mujoco Humanoid simulation (Adapted from [Gymnasium Documentation](https://gymnasium.farama.org/environments/mujoco/humanoid_standup/))\n",
        "The environment which is described in [Paper](https://ieeexplore.ieee.org/document/6386025) simulates a human consisting of two legs, two arms and a Torso.\n",
        "\n",
        "The environment is designet to learn a humanoid to stand up or to lern how to walk.\n",
        "\n",
        "### Overview\n",
        "| Description | Information |\n",
        "|----------|----------|\n",
        "| Action Space   | ``` Box(-0.4, 0.4, (17,), float32) ```   |\n",
        "| Observation Space    | ```Box(-inf, inf, (348,), float64)```     |\n",
        "| import learning to Walk    | \t```gymnasium.make(\"Humanoid-v4\")```     |\n",
        "| import standup   | ```gymnasium.make(\"HumanoidStandup-v4\")```     |\n",
        "\n",
        "\n",
        "### Action Space\n",
        "An action represents the torques applied at the hinge joints.\n",
        "\n",
        "![imgs/humanoid.png](imgs/humanoid.png)\n",
        "\n",
        "| Num | Name  | Joint | Type (Unit) | \n",
        "|----------|----------|----------|----------|\n",
        "| 0 | abdomen_y | hinge | torque (N m) |\n",
        "| 1 | abdomen_z | hinge | torque (N m) | \n",
        "| 2 | abdomen_x | hinge | torque (N m) |\n",
        "| 3 | right_hip_x | hinge | torque (N m) |\n",
        "| 4 | right_hip_z | hinge | torque (N m) |\n",
        "| 5 | right_hip_y | hinge | torque (N m) |\n",
        "| 6 | right_knee | hinge | torque (N m) | \n",
        "| 7 | left_hip_x  | hinge | torque (N m) |\n",
        "| 8 | left_hip_z | hinge | torque (N m) |\n",
        "| 9 | left_hip_y | hinge | torque (N m) | \n",
        "| 10 | left_knee | hinge | torque (N m) | \n",
        "| 11 | right_shoulder1 | hinge | torque (N m) | \n",
        "| 12 | right_shoulder2 | hinge | torque (N m) | \n",
        "| 13 | right_elbow | hinge | torque (N m) | \n",
        "| 14 | left_shoulder1 | hinge | torque (N m) | \n",
        "| 15 | left_shoulder2 | hinge | torque (N m) | \n",
        "| 16 | left_elbow | hinge | torque (N m) | \n",
        "\n",
        "### Observation Space\n",
        "\n",
        "### Rewards\n",
        "**Standup-Task**\n",
        "\n",
        "The total reward is: reward = uph_cost + 1 - quad_ctrl_cost - quad_impact_cost.\n",
        "- uph_cost: A reward for moving up\n",
        "- quad_ctrl_cost: A negative reward to penalize the Humanoid for taking actions that are too large.\n",
        "- impact_cost: A negative reward to penalize the Humanoid if the external contact forces are too large.\n",
        "\n",
        "**Learning to Walk-Task**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiPrsLZShX3t",
        "outputId": "d6243f84-df22-45c7-e829-1656409a14a9"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import multiprocessing\n",
        "\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3 import SAC, TD3, A2C, PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choice of the best RL Algorithm\n",
        "1. The basic idea is to test several RL algorithms with the standard parameters in parallel and to monitor the training progress using the reward.\n",
        "2. Subsequently, the algorithm that makes the fastest training progress is to be implemented and tuned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directories to hold models and logs for the tensor board\n",
        "model_dir = \"models\"\n",
        "log_dir = \"logs\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(log_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define training parameters\n",
        "TIMESTEPS = 25000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(env, humanoid_training_algo):\n",
        "    match humanoid_training_algo:\n",
        "        case 'PPO':\n",
        "            model = PPO('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case 'SAC':\n",
        "            model = SAC('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case 'TD3':\n",
        "            model = TD3('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case 'A2C':\n",
        "            model = A2C('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "        case _:\n",
        "            print('Algorithm not found')\n",
        "            return\n",
        "    iters = 0\n",
        "    while True:\n",
        "        iters += 1\n",
        "\n",
        "        model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False)\n",
        "        model.save(f\"{model_dir}/{humanoid_training_algo}_{TIMESTEPS*iters}\")\n",
        "\n",
        "def test(env, humanoid_training_algo, path_to_model):\n",
        "\n",
        "    match humanoid_training_algo:\n",
        "        case 'PPO':\n",
        "            model = PPO.load(path_to_model, env=env)\n",
        "        case 'SAC':\n",
        "            model = SAC.load(path_to_model, env=env)\n",
        "        case 'TD3':\n",
        "            model = TD3.load(path_to_model, env=env)\n",
        "        case 'A2C':\n",
        "            model = A2C.load(path_to_model, env=env)\n",
        "        case _:\n",
        "            print('Algorithm not found')\n",
        "            return\n",
        "\n",
        "    obs = env.reset()[0]\n",
        "    done = False\n",
        "    extra_steps = 500\n",
        "    while True:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, done, _, _ = env.step(action)\n",
        "\n",
        "        if done:\n",
        "            extra_steps -= 1\n",
        "\n",
        "            if extra_steps < 0:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"None\")\n",
        "algorithms = ['PPO', 'SAC', 'TD3', 'A2C']\n",
        "\n",
        "processes = []\n",
        "for algorithm in algorithms:\n",
        "    p = multiprocessing.Process(target=train, args=(env, algorithm,))\n",
        "    p.start()\n",
        "    processes.append(p)\n",
        "for p in processes:\n",
        "    p.join()\n",
        "\n",
        "# Start Tensorboard\n",
        "!tensorboard --logdir ./tensorboard/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_timesteps = 10000000\n",
        "n_steps = 5000\n",
        "learning_rate = 0.02\n",
        "batch_size = 512\n",
        "gamma = 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp_LdiqphilV",
        "outputId": "c6f5f1b6-3620-42a3-c841-072b3ab7b9fb"
      },
      "outputs": [],
      "source": [
        "experiment = Experiment(\n",
        "    api_key=\"eDVXm91zIoTyF8BUArQquxAmM\",\n",
        "    project_name=\"deeprl\",\n",
        "    workspace=\"clandolt\",\n",
        ")\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make(\"HumanoidStandup-v4\", render_mode=\"rgb_array\")\n",
        "env = gym.wrappers.RecordVideo(env, 'test')\n",
        "env = CometLogger(env, experiment)\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1, n_steps=n_steps, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma)\n",
        "model.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "# Save the agent\n",
        "model.save(\"ppo_humanoid\")\n",
        "vec_env = model.get_env()\n",
        "\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "# NOTE: if you have loading issue, you can pass `print_system_info=True`\n",
        "# to compare the system on which the model was trained vs the current one\n",
        "# model = DQN.load(\"dqn_lunar\", env=env, print_system_info=True)\n",
        "model = PPO.load(\"ppo_humanoid\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "# NOTE: If you use wrappers with your environment that modify rewards,\n",
        "#       this will be reflected here. To evaluate with original rewards,\n",
        "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = vec_env.step(action)\n",
        "    vec_env.render(\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ikpa4s9antl2"
      },
      "outputs": [],
      "source": [
        "experiment.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PYIe1wsLkDa-"
      },
      "outputs": [],
      "source": [
        "experiment.display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fOr0P1rMnkP9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
